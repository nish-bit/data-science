{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmYYpwqQu4du"
      },
      "outputs": [],
      "source": [
        "#RSS Feed Parsing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "\n",
        "rss_feeds = [\n",
        "    \"http://rss.cnn.com/rss/cnn_topstories.rss\",\n",
        "    \"http://qz.com/feed\",\n",
        "    \"http://feeds.foxnews.com/foxnews/politics\",\n",
        "    \"http://feeds.reuters.com/reuters/businessNews\",\n",
        "    \"http://feeds.feedburner.com/NewshourWorld\",\n",
        "    \"https://feeds.bbci.co.uk/news/world/asia/india/rss.xml\"\n",
        "]\n",
        "\n",
        "news_articles = []\n",
        "\n",
        "for feed_url in rss_feeds:\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    for entry in feed.entries:\n",
        "        news_articles.append({\n",
        "            'title': entry.title,\n",
        "            'link': entry.link,\n",
        "            'summary': entry.summary,\n",
        "            'published': entry.published,\n",
        "            'category': None  # This will be filled later\n",
        "        })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "xlzg4aQMvH0H",
        "outputId": "6401e5c5-054d-42bb-b26d-5b92c4885d06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'feedparser'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-28a0acd8c11c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m rss_feeds = [\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"http://rss.cnn.com/rss/cnn_topstories.rss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"http://qz.com/feed\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Categorization with NLTK:"
      ],
      "metadata": {
        "id": "x9HmBxdJvPkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import metrics\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample training data (you should replace this with a larger labeled dataset)\n",
        "training_data = [\n",
        "    ('Terrorism', 'Content related to terrorism and political unrest'),\n",
        "    ('Positive', 'Uplifting and positive news content'),\n",
        "    ('NaturalDisasters', 'News related to natural disasters'),\n",
        "    ('Others', 'Miscellaneous news content')\n",
        "    # Add more labeled examples as needed\n",
        "]\n",
        "\n",
        "# Tokenization, stop word removal, and stemming\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [stemmer.stem(word.lower()) for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Create a pipeline for text classification\n",
        "model = make_pipeline(\n",
        "    TfidfVectorizer(tokenizer=tokenize),\n",
        "    MultinomialNB()\n",
        ")\n",
        "\n",
        "# Split the training data\n",
        "X, y = zip(*training_data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict categories for news articles\n",
        "for article in news_articles:\n",
        "    article_text = article['title'] + ' ' + article['summary']\n",
        "    category = model.predict([article_text])[0]\n",
        "    article['category'] = category\n"
      ],
      "metadata": {
        "id": "U_-QydNuv92G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Database Storage"
      ],
      "metadata": {
        "id": "P4eyzYzKwID-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine, Column, Integer, String, DateTime\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from datetime import datetime\n",
        "\n",
        "Base = declarative_base()\n",
        "\n",
        "class NewsArticle(Base):\n",
        "    __tablename__ = 'news_articles'\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    title = Column(String)\n",
        "    link = Column(String)\n",
        "    summary = Column(String)\n",
        "    published = Column(DateTime)\n",
        "    category = Column(String)\n",
        "\n",
        "engine = create_engine('sqlite:///news_database.db')\n",
        "Base.metadata.create_all(engine)\n",
        "\n",
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()\n",
        "\n",
        "for article in news_articles:\n",
        "    db_article = NewsArticle(\n",
        "        title=article['title'],\n",
        "        link=article['link'],\n",
        "        summary=article['summary'],\n",
        "        published=datetime.strptime(article['published'], '%a, %d %b %Y %H:%M:%S %z'),\n",
        "        category=article['category']\n",
        "    )\n",
        "    session.add(db_article)\n",
        "\n",
        "session.commit()\n"
      ],
      "metadata": {
        "id": "5ZG_mOaQwO4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " #Further Enhancements:\n",
        "#Add error handling and logging for robustness.\n",
        "#Implement a web-based or command-line interface for users to interact with the application.\n",
        "#Schedule periodic updates to fetch new articles and update the database.\n",
        "#Consider using a more extensive labeled dataset for training the categorization model.\n",
        "#Remember to replace the sample training data with a more comprehensive and representative dataset for better\n",
        " #categorization accuracy. Additionally, you can fine-tune the NLTK-based categorization model or explore other machine\n",
        " #learning frameworks for more advanced text classification models"
      ],
      "metadata": {
        "id": "PeJsehKOwRHk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}